{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncate & Padding\n",
    "\n",
    "Assuming that our context $\\mathbf{D}$ and question $\\mathbf{Q}$ are already converted from token to ids, saved in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truncate(filepath,ml=1000):\n",
    "    \"\"\"truncate at max length, ignore if shorter\n",
    "    \"\"\"\n",
    "    c = []\n",
    "    with open(filepath,'r') as f:\n",
    "        for line in f:\n",
    "            c.append([int(item) for i,item in enumerate(line.strip().split()) if i<ml])\n",
    "    return c\n",
    "\n",
    "def padding(c):   #padding\n",
    "    \"\"\"padding with 0 w.r.t max length \n",
    "    \"\"\"\n",
    "    max_l = max([len(i) for i in c])\n",
    "    for i in range(len(c)):\n",
    "        c[i] = c[i] + (max_l-len(c[i]))*[0]\n",
    "    return c, max_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "D_data,max_l_D = padding(truncate('data/train.ids.context',600))\n",
    "Q_data,max_l_Q = padding(truncate('data/train.ids.question'))\n",
    "A_data = truncate('data/train.span')\n",
    "A_start_data = [i[0] for i in A_data]\n",
    "A_end_data = [i[1] for i in A_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $\\mathbf{D}$ and $\\mathbf{Q}$ length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Document $\\mathbf{D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f47f9e7af60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEKCAYAAAArYJMgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f480b75d668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('data/train.ids.context', 'r') as f:\n",
    "    result = []\n",
    "    for line in f:\n",
    "        result.append(len(line.split()))\n",
    "\n",
    "pd.Series(data=result).hist(bins=50)\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Document $\\mathbf{Q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f47fd1d75c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f480b76a710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('data/train.ids.question', 'r') as f:\n",
    "    result = []\n",
    "    for line in f:\n",
    "        result.append(len(line.split()))\n",
    "\n",
    "pd.Series(data=result).hist(bins=50)\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "print (max_l_D)\n",
    "print (max_l_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "    \"\"\"calculate number of non zero values in vector\n",
    "    \"\"\"\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), axis=2))\n",
    "    length = tf.reduce_sum(used, axis=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(question,document,embeddings,hidden_units=200):\n",
    "    \"\"\"build encoder\n",
    "    \n",
    "    question: Q\n",
    "    document: D\n",
    "    embeddings: GloVe\n",
    "    hidden_units: hidden units\n",
    "    \n",
    "    return U\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(question)[0]\n",
    "    # get question and document GloVe embedding\n",
    "    q_embedding = tf.nn.embedding_lookup(embeddings,question)\n",
    "    d_embedding = tf.nn.embedding_lookup(embeddings,document)\n",
    "    \n",
    "#     print (q_embedding.shape,\"=?,60,100\")\n",
    "#     print (d_embedding.shape,\"=?,600,100\")\n",
    "    \n",
    "    lstm_enc = tf.contrib.rnn.BasicLSTMCell(hidden_units)\n",
    "    \n",
    "    with tf.variable_scope('document_encoder') as scope1:\n",
    "        document_states,_ = tf.nn.dynamic_rnn(cell=lstm_enc,\n",
    "                                              dtype=tf.float32,\n",
    "                                              inputs=d_embedding,\n",
    "                                              sequence_length=length(q_embedding),\n",
    "                                              time_major=False)\n",
    "        \n",
    "    with tf.variable_scope('question_encoder') as scope2:\n",
    "        question_states,_ = tf.nn.dynamic_rnn(cell=lstm_enc,\n",
    "                                              dtype=tf.float32,\n",
    "                                              inputs=q_embedding,\n",
    "                                              sequence_length=length(d_embedding),\n",
    "                                              time_major=False)\n",
    "\n",
    "    # perform additional MLP to encode Q\n",
    "    Wq = tf.get_variable(name=\"Wq\",\n",
    "                         shape=[hidden_units,hidden_units],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                         dtype=tf.float32)\n",
    "    Wq = tf.expand_dims(tf.ones([batch_size,1]), 1) * Wq\n",
    "    bq = tf.Variable(name='bq',\n",
    "                     initial_value=tf.constant(0.0,shape=[hidden_units,]),\n",
    "                     dtype=tf.float32)\n",
    "\n",
    "    print (document_states.shape,\"=?,600,200\")\n",
    "    print (question_states.shape,\"=?,60,200\")\n",
    "    print (Wq.shape,\"=?,200,200\")\n",
    "    print (bq.shape,\"=200,\")\n",
    "                    \n",
    "    question_states_modified_duplicate = tf.nn.tanh(tf.matmul(question_states,Wq)+bq)\n",
    "    question_states_modified = tf.transpose(question_states_modified_duplicate,perm=[0,2,1]) \n",
    "    \n",
    "    print (question_states_modified.shape,\"=?,200,60\")\n",
    "    print (question_states_modified_duplicate.shape,\"=?,60,200\")\n",
    "\n",
    "    # coattention encoder\n",
    "    \n",
    "    l = tf.matmul(document_states,question_states_modified)\n",
    "    print (l.shape,\"=?,600,60\")\n",
    "    aq = tf.nn.softmax(l)\n",
    "    ad = tf.nn.softmax(tf.transpose(l,perm=[0, 2, 1]))\n",
    "    print (aq.shape,\"=?,600,60\")\n",
    "    print (ad.shape,\"=?,60,600\")\n",
    "    \n",
    "    cq = tf.matmul(tf.transpose(aq,perm=[0,2,1]),document_states)\n",
    "    print (cq.shape,\"=?,60,200\")\n",
    "    print (question_states_modified_duplicate.shape,\"=?,60,200\")\n",
    "    qcq = tf.concat([question_states_modified_duplicate,cq],2)\n",
    "    print (qcq.shape,\"=?,60,400\")\n",
    "    cd = (tf.matmul(tf.transpose(ad,perm=[0,2,1]),qcq))\n",
    "    print (cd.shape,\"=?,600,400\")\n",
    "    dcd = tf.concat([document_states,cd],axis=2)\n",
    "    print (dcd.shape,\"=?,600,600\")\n",
    "    \n",
    "    with tf.variable_scope('coattention'):\n",
    "        u_lstm_fw = tf.contrib.rnn.BasicLSTMCell(hidden_units)  #bi-lstm\n",
    "        u_lstm_bw = tf.contrib.rnn.BasicLSTMCell(hidden_units)\n",
    "        u_states,_ = tf.nn.bidirectional_dynamic_rnn(cell_bw=u_lstm_bw,cell_fw=u_lstm_fw,dtype=tf.float32,inputs=dcd,time_major=False,sequence_length=length(dcd))\n",
    "    encoder_states = tf.concat(u_states,2) # concat [fw;bw] to make U\n",
    "    print (encoder_states.shape)\n",
    "    return encoder_states # U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(knowledge_reps,hidden_units = 200):\n",
    "    \"\"\"extract and predict i_start, i_end respectively from U\n",
    "    \n",
    "    knowledge_reps: coattention encoding U\n",
    "    hidden_units: hidden units\n",
    "    \n",
    "    return: start_vector, end_vector, start_logits, end_logits\n",
    "    \"\"\"\n",
    "    \n",
    "    # randomly initialise s and e\n",
    "    batch_size = tf.shape(knowledge_reps)[0]\n",
    "#     print(f\"batch_size = {batch_size}\")\n",
    "    pool = 16\n",
    "    e = np.random.randint(max_l_D) + 1\n",
    "    s = np.random.randint(e)\n",
    "    sv = tf.tile([s],[batch_size])\n",
    "    ev = tf.tile([e],[batch_size])\n",
    "\n",
    "    # lstm cell\n",
    "    lstm_dec = tf.contrib.rnn.LSTMCell(hidden_units)\n",
    "    ch = lstm_dec.zero_state(batch_size,dtype=tf.float32)\n",
    "    hi,ci = ch\n",
    "    \n",
    "    with tf.variable_scope('hmn1') as scope1:\n",
    "        wd = tf.get_variable(name=\"wd\",\n",
    "                             shape=[hidden_units,5*hidden_units],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             dtype=tf.float32)\n",
    "        w1 = tf.get_variable(name=\"w1\",\n",
    "                             shape=[pool*hidden_units,3*hidden_units],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             dtype=tf.float32)\n",
    "        w2 = tf.get_variable(name=\"w2\",\n",
    "                             shape=[pool*hidden_units,hidden_units],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             dtype=tf.float32)\n",
    "        w3 = tf.get_variable(name=\"w3\",\n",
    "                             shape=[pool,2*hidden_units],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('hmn2') as scope2:\n",
    "        wd = tf.get_variable(name=\"wd\",\n",
    "                             shape=[hidden_units,5*hidden_units],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             dtype=tf.float32)\n",
    "        w1 = tf.get_variable(name=\"w1\",\n",
    "                             shape=[pool*hidden_units,3*hidden_units],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             dtype=tf.float32)\n",
    "        w2 = tf.get_variable(name=\"w2\",\n",
    "                             shape=[pool*hidden_units,hidden_units],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             dtype=tf.float32)\n",
    "        w3 = tf.get_variable(name=\"w3\",\n",
    "                             shape=[pool,2*hidden_units],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             dtype=tf.float32)\n",
    "        \n",
    "    #loop 4 times to call lstm cell to predict sv, ev:\n",
    "    for i in range(4):\n",
    "        #concatenate u_s and u_e\n",
    "        u_s = tf.gather_nd(params=knowledge_reps,indices=tf.stack([tf.range(batch_size,dtype=tf.int32),sv],axis=1))\n",
    "        u_e = tf.gather_nd(params=knowledge_reps,indices=tf.stack([tf.range(batch_size,dtype=tf.int32),ev],axis=1))\n",
    "        usue = tf.concat([u_s,u_e],axis=1)\n",
    "        \n",
    "        print (i,usue.shape,hi.shape)\n",
    "        \n",
    "        #calculate hi        \n",
    "        with tf.variable_scope(\"hmn1\",reuse=True) as scope1:\n",
    "            sv,hmns_output = hmn(knowledge_reps,hi,u_s,u_e,hidden_units,pool)#loop over the document length times to obtain alpha t using HNM function\n",
    "        with tf.variable_scope(\"hmn2\",reuse=True) as scope2:\n",
    "            ev,hmne_output = hmn(knowledge_reps,hi,u_s,u_e,hidden_units,pool)#loop over the document length times to obtain beta t using HNM function\n",
    "        \n",
    "        hi,ch = lstm_dec(inputs=usue,state=ch) \n",
    "        \n",
    "    return sv,ev,hmns_output,hmne_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hmn(ut,hs,us,ue,hidden_units,pool=16):\n",
    "    \n",
    "    #calculate r\n",
    "    wd = tf.get_variable(name=\"wd\",\n",
    "                         shape=[hidden_units,5*hidden_units],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                         dtype=tf.float32)\n",
    "    x = tf.concat([hs,us,ue],axis=1)\n",
    "    r = tf.nn.tanh(tf.matmul(x,tf.transpose(wd)))\n",
    "\n",
    "    #calculate mt1\n",
    "    r1 = tf.expand_dims(tf.ones([int(ut.shape[1]),1]), 1) * r\n",
    "    r1 = tf.reshape(r1,[-1,hidden_units])\n",
    "    ut1 = tf.reshape(ut,[-1,2*hidden_units])\n",
    "    ut2 = tf.concat([ut1,r1],axis=1)\n",
    "    w1 = tf.get_variable(name=\"w1\",\n",
    "                         shape=[pool*hidden_units,3*hidden_units],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                         dtype=tf.float32)\n",
    "    b1 = tf.Variable(tf.constant(0.0,shape=[pool*hidden_units,]),dtype=tf.float32)\n",
    "    x1 = tf.matmul(ut2,tf.transpose(w1))+b1\n",
    "    x1 = tf.reshape(x1,[-1,pool])\n",
    "    x1 = tf.reduce_max(x1,axis=1)\n",
    "    mt1 = tf.reshape(x1,[-1,hidden_units])\n",
    "    \n",
    "    #calculate mt2\n",
    "    w2 = tf.get_variable(name=\"w2\",\n",
    "                         shape=[pool*hidden_units,hidden_units],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                         dtype=tf.float32)\n",
    "    b2 = tf.Variable(tf.constant(0.0,shape=[pool*hidden_units,]),dtype=tf.float32)\n",
    "    x2 = tf.matmul(mt1,tf.transpose(w2))+b2\n",
    "    x2 = tf.reshape(x2,[-1,pool])\n",
    "    x2 = tf.reduce_max(x2,axis=1)\n",
    "    mt2 = tf.reshape(x2,[-1,hidden_units])\n",
    "    \n",
    "    #max\n",
    "    mt1mt2 = tf.concat([mt1,mt2],axis=1)\n",
    "    w3 = tf.get_variable(name=\"w3\",\n",
    "                         shape=[pool,2*hidden_units],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                         dtype=tf.float32)\n",
    "    b3 = tf.Variable(tf.constant(0.0,shape=[pool,]),dtype=tf.float32)\n",
    "    x3 = tf.matmul(mt1mt2,tf.transpose(w3))+b3\n",
    "    x3 = tf.reduce_max(x3,axis=1)\n",
    "    hmnout = tf.reshape(x3,[-1,int(ut.shape[1])])\n",
    "\n",
    "    #argmax\n",
    "    maxout = tf.argmax(hmnout,axis=1)\n",
    "    maxout = tf.cast(maxout,dtype=tf.int32)\n",
    "    \n",
    "    return maxout,hmnout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read embedding file\n",
    "embedding_array = np.load('data/glove.trimmed.100.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115240, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_array['glove'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 600, 200) =?,600,200\n",
      "(?, 60, 200) =?,60,200\n",
      "(?, 200, 200) =?,200,200\n",
      "(200,) =200,\n",
      "(?, 200, 60) =?,200,60\n",
      "(?, 60, 200) =?,60,200\n",
      "(?, 600, 60) =?,600,60\n",
      "(?, 600, 60) =?,600,60\n",
      "(?, 60, 600) =?,60,600\n",
      "(?, 60, 200) =?,60,200\n",
      "(?, 60, 200) =?,60,200\n",
      "(?, 60, 400) =?,60,400\n",
      "(?, 600, 400) =?,600,400\n",
      "(?, 600, 600) =?,600,600\n",
      "(?, 600, 400)\n",
      "decoder starts\n",
      "0 (?, 800) (?, 200)\n",
      "1 (?, 800) (?, 200)\n",
      "2 (?, 800) (?, 200)\n",
      "3 (?, 800) (?, 200)\n"
     ]
    }
   ],
   "source": [
    "## create placeholders\n",
    "tf.reset_default_graph()\n",
    "hidden_units = 200\n",
    "question = tf.placeholder(dtype=tf.int32,shape=[None,max_l_Q])\n",
    "context = tf.placeholder(dtype=tf.int32,shape=[None,max_l_D])\n",
    "answer_start = tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "answer_end = tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "embeddings = tf.constant(embedding_array['glove'],dtype=tf.float32)\n",
    "\n",
    "encoder_states = encoder(question,context,embeddings)\n",
    "print (\"decoder starts\")\n",
    "decoder_output_start, decoder_output_end, hmns_output, hmne_output = decoder(encoder_states)\n",
    "\n",
    "## add loss\n",
    "l1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=answer_start,logits=hmns_output)\n",
    "l2 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=answer_end,logits=hmne_output)\n",
    "loss = l1 + l2\n",
    "\n",
    "## predictions\n",
    "pred_start = tf.argmax(hmns_output, 1)\n",
    "pred_end = tf.argmax(hmne_output, 1)\n",
    "\n",
    "## add optimizer\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Exact Matching (EM) and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99927521098731009, 1.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def evaluate(context, predict_start, predict_end, answer_start, answer_end):\n",
    "    f1_scores = []\n",
    "    em_scores = []\n",
    "    for i, vector in enumerate(context):\n",
    "        prediction_tokens = vector[predict_start[i]:predict_end[i]+1]\n",
    "        ground_truth_tokens = vector[answer_start[i]:answer_end[i]+1]\n",
    "        common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            f1_scores.append(0)\n",
    "            continue\n",
    "        precision = 1.0 * num_same / len(prediction_tokens)\n",
    "        recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        em = set(prediction_tokens) == set(ground_truth_tokens)\n",
    "        f1_scores.append(f1)\n",
    "        em_scores.append(em)\n",
    "    return np.mean(f1_scores), np.mean(em_scores)\n",
    "\n",
    "evaluate(D_data, A_start_data, A_end_data,A_start_data, A_end_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635.9609375"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D_data) / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialise variables and train\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "step =  0\n",
      "F1 Score =  0.0257043899373\n",
      "EM score =  0.0\n",
      "loss_train =  12.7627\n",
      "====================\n",
      "Validation Step =  0\n",
      "Validation F1 Score =  0.0116003203469\n",
      "Validation EM Score =  0.0\n",
      "Validation loss =  12.5466\n",
      "--------------------\n",
      "step =  1\n",
      "F1 Score =  0.0218750360715\n",
      "EM score =  0.0\n",
      "loss_train =  12.5125\n",
      "--------------------\n",
      "step =  2\n",
      "F1 Score =  0.0115918803419\n",
      "EM score =  0.0\n",
      "loss_train =  11.616\n",
      "--------------------\n",
      "step =  3\n",
      "F1 Score =  0.0303013392857\n",
      "EM score =  0.0\n",
      "loss_train =  12.6719\n",
      "--------------------\n",
      "step =  4\n",
      "F1 Score =  0.0277937043768\n",
      "EM score =  0.0\n",
      "loss_train =  11.4401\n",
      "--------------------\n",
      "step =  5\n",
      "F1 Score =  0.0324644134497\n",
      "EM score =  0.0\n",
      "loss_train =  11.4763\n",
      "--------------------\n",
      "step =  6\n",
      "F1 Score =  0.0235225558755\n",
      "EM score =  0.0\n",
      "loss_train =  11.6627\n",
      "--------------------\n",
      "step =  7\n",
      "F1 Score =  0.0267647372981\n",
      "EM score =  0.0\n",
      "loss_train =  11.8135\n",
      "--------------------\n",
      "step =  8\n",
      "F1 Score =  0.0251674107143\n",
      "EM score =  0.142857142857\n",
      "loss_train =  11.2959\n",
      "--------------------\n",
      "step =  9\n",
      "F1 Score =  0.0152599447336\n",
      "EM score =  0.0\n",
      "loss_train =  10.8678\n",
      "--------------------\n",
      "step =  10\n",
      "F1 Score =  0.0212739010806\n",
      "EM score =  0.0\n",
      "loss_train =  10.7386\n",
      "--------------------\n",
      "step =  11\n",
      "F1 Score =  0.0333277053052\n",
      "EM score =  0.0\n",
      "loss_train =  10.5443\n",
      "--------------------\n",
      "step =  12\n",
      "F1 Score =  0.050673223835\n",
      "EM score =  0.0714285714286\n",
      "loss_train =  10.6923\n",
      "--------------------\n",
      "step =  13\n",
      "F1 Score =  0.0366125761627\n",
      "EM score =  0.0\n",
      "loss_train =  10.5842\n",
      "--------------------\n",
      "step =  14\n",
      "F1 Score =  0.0430512547367\n",
      "EM score =  0.0588235294118\n",
      "loss_train =  10.8446\n",
      "--------------------\n",
      "step =  15\n",
      "F1 Score =  0.0191168666821\n",
      "EM score =  0.0\n",
      "loss_train =  10.7194\n",
      "--------------------\n",
      "step =  16\n",
      "F1 Score =  0.0106772109886\n",
      "EM score =  0.0\n",
      "loss_train =  10.5911\n",
      "--------------------\n",
      "step =  17\n",
      "F1 Score =  0.0177533361679\n",
      "EM score =  0.0\n",
      "loss_train =  10.7046\n",
      "--------------------\n",
      "step =  18\n",
      "F1 Score =  0.0308953920739\n",
      "EM score =  0.0\n",
      "loss_train =  11.4237\n",
      "--------------------\n",
      "step =  19\n",
      "F1 Score =  0.00722429661368\n",
      "EM score =  0.0\n",
      "loss_train =  10.8728\n",
      "--------------------\n",
      "step =  20\n",
      "F1 Score =  0.0163207497911\n",
      "EM score =  0.0\n",
      "loss_train =  10.9162\n",
      "--------------------\n",
      "step =  21\n",
      "F1 Score =  0.00662727029915\n",
      "EM score =  0.0\n",
      "loss_train =  10.6475\n",
      "--------------------\n",
      "step =  22\n",
      "F1 Score =  0.0233691487819\n",
      "EM score =  0.0\n",
      "loss_train =  10.0521\n",
      "--------------------\n",
      "step =  23\n",
      "F1 Score =  0.039286838253\n",
      "EM score =  0.0\n",
      "loss_train =  10.0813\n",
      "--------------------\n",
      "step =  24\n",
      "F1 Score =  0.0613007318127\n",
      "EM score =  0.0454545454545\n",
      "loss_train =  10.874\n",
      "--------------------\n",
      "step =  25\n",
      "F1 Score =  0.0264409590869\n",
      "EM score =  0.1\n",
      "loss_train =  10.573\n",
      "--------------------\n",
      "step =  26\n",
      "F1 Score =  0.0607143211309\n",
      "EM score =  0.0\n",
      "loss_train =  10.4058\n",
      "--------------------\n",
      "step =  27\n",
      "F1 Score =  0.0496781346874\n",
      "EM score =  0.0454545454545\n",
      "loss_train =  10.023\n",
      "--------------------\n",
      "step =  28\n",
      "F1 Score =  0.0301602926525\n",
      "EM score =  0.0\n",
      "loss_train =  11.8692\n",
      "--------------------\n",
      "step =  29\n",
      "F1 Score =  0.0448414775694\n",
      "EM score =  0.0588235294118\n",
      "loss_train =  11.4286\n",
      "--------------------\n",
      "step =  30\n",
      "F1 Score =  0.0749692113548\n",
      "EM score =  0.0740740740741\n",
      "loss_train =  10.4861\n",
      "--------------------\n",
      "step =  31\n",
      "F1 Score =  0.0488066602438\n",
      "EM score =  0.0\n",
      "loss_train =  10.4885\n",
      "--------------------\n",
      "step =  32\n",
      "F1 Score =  0.0400969503789\n",
      "EM score =  0.0\n",
      "loss_train =  10.4036\n",
      "--------------------\n",
      "step =  33\n",
      "F1 Score =  0.0275401985569\n",
      "EM score =  0.0\n",
      "loss_train =  10.4582\n",
      "--------------------\n",
      "step =  34\n",
      "F1 Score =  0.0883537250596\n",
      "EM score =  0.0967741935484\n",
      "loss_train =  10.366\n",
      "--------------------\n",
      "step =  35\n",
      "F1 Score =  0.0544825834401\n",
      "EM score =  0.03125\n",
      "loss_train =  10.3555\n",
      "--------------------\n",
      "step =  36\n",
      "F1 Score =  0.0676184967543\n",
      "EM score =  0.0232558139535\n",
      "loss_train =  9.94169\n",
      "--------------------\n",
      "step =  37\n",
      "F1 Score =  0.0429461540892\n",
      "EM score =  0.0952380952381\n",
      "loss_train =  10.1757\n",
      "--------------------\n",
      "step =  38\n",
      "F1 Score =  0.0463125421153\n",
      "EM score =  0.0416666666667\n",
      "loss_train =  10.0656\n",
      "--------------------\n",
      "step =  39\n",
      "F1 Score =  0.0391547791357\n",
      "EM score =  0.0555555555556\n",
      "loss_train =  10.2674\n",
      "--------------------\n",
      "step =  40\n",
      "F1 Score =  0.0639625347524\n",
      "EM score =  0.0384615384615\n",
      "loss_train =  10.0237\n",
      "--------------------\n",
      "step =  41\n",
      "F1 Score =  0.0456545721394\n",
      "EM score =  0.0588235294118\n",
      "loss_train =  10.3708\n",
      "--------------------\n",
      "step =  42\n",
      "F1 Score =  0.0797203558561\n",
      "EM score =  0.166666666667\n",
      "loss_train =  10.3009\n",
      "--------------------\n",
      "step =  43\n",
      "F1 Score =  0.0587539124014\n",
      "EM score =  0.157894736842\n",
      "loss_train =  9.96825\n",
      "--------------------\n",
      "step =  44\n",
      "F1 Score =  0.0513014793986\n",
      "EM score =  0.0555555555556\n",
      "loss_train =  10.1429\n",
      "--------------------\n",
      "step =  45\n",
      "F1 Score =  0.063766755966\n",
      "EM score =  0.181818181818\n",
      "loss_train =  10.1346\n",
      "--------------------\n",
      "step =  46\n",
      "F1 Score =  0.0520257136302\n",
      "EM score =  0.0344827586207\n",
      "loss_train =  10.3938\n",
      "--------------------\n",
      "step =  47\n",
      "F1 Score =  0.0369960909788\n",
      "EM score =  0.0625\n",
      "loss_train =  10.1927\n",
      "--------------------\n",
      "step =  48\n",
      "F1 Score =  0.0661483079339\n",
      "EM score =  0.0384615384615\n",
      "loss_train =  10.5779\n",
      "--------------------\n",
      "step =  49\n",
      "F1 Score =  0.0619993600003\n",
      "EM score =  0.142857142857\n",
      "loss_train =  10.3474\n",
      "--------------------\n",
      "step =  50\n",
      "F1 Score =  0.0522338509055\n",
      "EM score =  0.15\n",
      "loss_train =  10.2257\n",
      "--------------------\n",
      "step =  51\n",
      "F1 Score =  0.0494151617452\n",
      "EM score =  0.142857142857\n",
      "loss_train =  10.7841\n",
      "--------------------\n",
      "step =  52\n",
      "F1 Score =  0.0478258097408\n",
      "EM score =  0.0384615384615\n",
      "loss_train =  10.2914\n",
      "--------------------\n",
      "step =  53\n",
      "F1 Score =  0.0427267952775\n",
      "EM score =  0.0\n",
      "loss_train =  10.2421\n",
      "--------------------\n",
      "step =  54\n",
      "F1 Score =  0.0715495458258\n",
      "EM score =  0.0857142857143\n",
      "loss_train =  9.86734\n",
      "--------------------\n",
      "step =  55\n",
      "F1 Score =  0.0409815417094\n",
      "EM score =  0.0\n",
      "loss_train =  9.96597\n",
      "--------------------\n",
      "step =  56\n",
      "F1 Score =  0.0576467124507\n",
      "EM score =  0.0526315789474\n",
      "loss_train =  9.8599\n",
      "--------------------\n",
      "step =  57\n",
      "F1 Score =  0.0746886918148\n",
      "EM score =  0.115384615385\n",
      "loss_train =  10.4553\n",
      "--------------------\n",
      "step =  58\n",
      "F1 Score =  0.0687697207081\n",
      "EM score =  0.0454545454545\n",
      "loss_train =  9.27354\n",
      "--------------------\n",
      "step =  59\n",
      "F1 Score =  0.0441506100546\n",
      "EM score =  0.0833333333333\n",
      "loss_train =  10.2036\n",
      "--------------------\n",
      "step =  60\n",
      "F1 Score =  0.101285498042\n",
      "EM score =  0.208333333333\n",
      "loss_train =  9.84756\n",
      "--------------------\n",
      "step =  61\n",
      "F1 Score =  0.0587825664291\n",
      "EM score =  0.166666666667\n",
      "loss_train =  9.95298\n",
      "--------------------\n",
      "step =  62\n",
      "F1 Score =  0.0513133846375\n",
      "EM score =  0.117647058824\n",
      "loss_train =  10.1396\n",
      "--------------------\n",
      "step =  63\n",
      "F1 Score =  0.0615188899731\n",
      "EM score =  0.111111111111\n",
      "loss_train =  9.69217\n",
      "--------------------\n",
      "step =  64\n",
      "F1 Score =  0.0834201388889\n",
      "EM score =  0.263157894737\n",
      "loss_train =  9.83392\n",
      "--------------------\n",
      "step =  65\n",
      "F1 Score =  0.107777388242\n",
      "EM score =  0.103448275862\n",
      "loss_train =  9.41424\n",
      "--------------------\n",
      "step =  66\n",
      "F1 Score =  0.0369008559301\n",
      "EM score =  0.111111111111\n",
      "loss_train =  9.96618\n",
      "--------------------\n",
      "step =  67\n",
      "F1 Score =  0.083202392937\n",
      "EM score =  0.192307692308\n",
      "loss_train =  9.50663\n",
      "--------------------\n",
      "step =  68\n",
      "F1 Score =  0.0487677530507\n",
      "EM score =  0.0625\n",
      "loss_train =  9.56734\n",
      "--------------------\n",
      "step =  69\n",
      "F1 Score =  0.0585655194283\n",
      "EM score =  0.166666666667\n",
      "loss_train =  9.35114\n",
      "--------------------\n",
      "step =  70\n",
      "F1 Score =  0.0733764878572\n",
      "EM score =  0.0689655172414\n",
      "loss_train =  9.36842\n",
      "--------------------\n",
      "step =  71\n",
      "F1 Score =  0.0546634218953\n",
      "EM score =  0.0645161290323\n",
      "loss_train =  9.79919\n",
      "--------------------\n",
      "step =  72\n",
      "F1 Score =  0.0667806580217\n",
      "EM score =  0.0769230769231\n",
      "loss_train =  9.75373\n",
      "--------------------\n",
      "step =  73\n",
      "F1 Score =  0.0777266282947\n",
      "EM score =  0.0357142857143\n",
      "loss_train =  9.44202\n",
      "--------------------\n",
      "step =  74\n",
      "F1 Score =  0.0690407325945\n",
      "EM score =  0.0967741935484\n",
      "loss_train =  9.70968\n",
      "--------------------\n",
      "step =  75\n",
      "F1 Score =  0.0538585019847\n",
      "EM score =  0.0\n",
      "loss_train =  9.51181\n",
      "--------------------\n",
      "step =  76\n",
      "F1 Score =  0.102374084046\n",
      "EM score =  0.05\n",
      "loss_train =  9.22127\n",
      "--------------------\n",
      "step =  77\n",
      "F1 Score =  0.0766073584539\n",
      "EM score =  0.0645161290323\n",
      "loss_train =  9.43928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "step =  78\n",
      "F1 Score =  0.072608346667\n",
      "EM score =  0.030303030303\n",
      "loss_train =  9.25807\n",
      "--------------------\n",
      "step =  79\n",
      "F1 Score =  0.049110820087\n",
      "EM score =  0.037037037037\n",
      "loss_train =  9.24102\n",
      "--------------------\n",
      "step =  80\n",
      "F1 Score =  0.0514127241986\n",
      "EM score =  0.0\n",
      "loss_train =  9.38033\n",
      "--------------------\n",
      "step =  81\n",
      "F1 Score =  0.0685310251489\n",
      "EM score =  0.181818181818\n",
      "loss_train =  9.3267\n",
      "--------------------\n",
      "step =  82\n",
      "F1 Score =  0.0581615191408\n",
      "EM score =  0.0689655172414\n",
      "loss_train =  9.26033\n",
      "--------------------\n",
      "step =  83\n",
      "F1 Score =  0.0523402595813\n",
      "EM score =  0.0384615384615\n",
      "loss_train =  9.39788\n",
      "--------------------\n",
      "step =  84\n",
      "F1 Score =  0.0282705190139\n",
      "EM score =  0.0\n",
      "loss_train =  9.46683\n",
      "--------------------\n",
      "step =  85\n",
      "F1 Score =  0.0501563245393\n",
      "EM score =  0.0\n",
      "loss_train =  9.34104\n",
      "--------------------\n",
      "step =  86\n",
      "F1 Score =  0.0374557299769\n",
      "EM score =  0.0\n",
      "loss_train =  9.80237\n",
      "--------------------\n",
      "step =  87\n",
      "F1 Score =  0.0563014372335\n",
      "EM score =  0.0625\n",
      "loss_train =  9.21647\n",
      "--------------------\n",
      "step =  88\n",
      "F1 Score =  0.0684030495281\n",
      "EM score =  0.103448275862\n",
      "loss_train =  9.45009\n",
      "--------------------\n",
      "step =  89\n",
      "F1 Score =  0.0552809387187\n",
      "EM score =  0.0666666666667\n",
      "loss_train =  9.72427\n",
      "--------------------\n",
      "step =  90\n",
      "F1 Score =  0.0443426864025\n",
      "EM score =  0.0322580645161\n",
      "loss_train =  9.31031\n",
      "--------------------\n",
      "step =  91\n",
      "F1 Score =  0.0739579501827\n",
      "EM score =  0.1\n",
      "loss_train =  9.54745\n",
      "--------------------\n",
      "step =  92\n",
      "F1 Score =  0.0718921244843\n",
      "EM score =  0.0\n",
      "loss_train =  9.31206\n",
      "--------------------\n",
      "step =  93\n",
      "F1 Score =  0.0643362438548\n",
      "EM score =  0.0909090909091\n",
      "loss_train =  9.24324\n",
      "--------------------\n",
      "step =  94\n",
      "F1 Score =  0.0486154490282\n",
      "EM score =  0.0333333333333\n",
      "loss_train =  8.99743\n",
      "--------------------\n",
      "step =  95\n",
      "F1 Score =  0.0588370092811\n",
      "EM score =  0.0740740740741\n",
      "loss_train =  9.80122\n",
      "--------------------\n",
      "step =  96\n",
      "F1 Score =  0.0687193779799\n",
      "EM score =  0.103448275862\n",
      "loss_train =  9.41181\n",
      "--------------------\n",
      "step =  97\n",
      "F1 Score =  0.0782816637377\n",
      "EM score =  0.0540540540541\n",
      "loss_train =  9.46563\n",
      "--------------------\n",
      "step =  98\n",
      "F1 Score =  0.0889414048134\n",
      "EM score =  0.102564102564\n",
      "loss_train =  9.17844\n",
      "--------------------\n",
      "step =  99\n",
      "F1 Score =  0.0314003288889\n",
      "EM score =  0.0\n",
      "loss_train =  9.703\n",
      "--------------------\n",
      "step =  100\n",
      "F1 Score =  0.0601616405723\n",
      "EM score =  0.0322580645161\n",
      "loss_train =  8.79481\n",
      "====================\n",
      "Validation Step =  100\n",
      "Validation F1 Score =  0.0531521668728\n",
      "Validation EM Score =  0.0769230769231\n",
      "Validation loss =  9.49266\n",
      "--------------------\n",
      "step =  101\n",
      "F1 Score =  0.102357790803\n",
      "EM score =  0.179487179487\n",
      "loss_train =  9.1282\n",
      "--------------------\n",
      "step =  102\n",
      "F1 Score =  0.07686446397\n",
      "EM score =  0.142857142857\n",
      "loss_train =  9.00188\n",
      "--------------------\n",
      "step =  103\n",
      "F1 Score =  0.0612152700592\n",
      "EM score =  0.030303030303\n",
      "loss_train =  9.10893\n",
      "--------------------\n",
      "step =  104\n",
      "F1 Score =  0.0596078579857\n",
      "EM score =  0.0294117647059\n",
      "loss_train =  9.42576\n",
      "--------------------\n",
      "step =  105\n",
      "F1 Score =  0.0398464872483\n",
      "EM score =  0.0\n",
      "loss_train =  9.61798\n",
      "--------------------\n",
      "step =  106\n",
      "F1 Score =  0.0932080881051\n",
      "EM score =  0.0833333333333\n",
      "loss_train =  9.04008\n",
      "--------------------\n",
      "step =  107\n",
      "F1 Score =  0.0800466965132\n",
      "EM score =  0.046511627907\n",
      "loss_train =  9.1572\n",
      "--------------------\n",
      "step =  108\n",
      "F1 Score =  0.0301573821367\n",
      "EM score =  0.0\n",
      "loss_train =  9.19718\n",
      "--------------------\n",
      "step =  109\n",
      "F1 Score =  0.0524259985153\n",
      "EM score =  0.0740740740741\n",
      "loss_train =  9.38668\n",
      "--------------------\n",
      "step =  110\n",
      "F1 Score =  0.096523620255\n",
      "EM score =  0.0789473684211\n",
      "loss_train =  9.07571\n",
      "--------------------\n",
      "step =  111\n",
      "F1 Score =  0.0939175724072\n",
      "EM score =  0.142857142857\n",
      "loss_train =  9.16438\n",
      "--------------------\n",
      "step =  112\n",
      "F1 Score =  0.0793364142836\n",
      "EM score =  0.129032258065\n",
      "loss_train =  9.13641\n",
      "--------------------\n",
      "step =  113\n",
      "F1 Score =  0.0720122446072\n",
      "EM score =  0.0625\n",
      "loss_train =  9.04469\n",
      "--------------------\n",
      "step =  114\n",
      "F1 Score =  0.0735860447484\n",
      "EM score =  0.0909090909091\n",
      "loss_train =  9.13518\n",
      "--------------------\n",
      "step =  115\n",
      "F1 Score =  0.0604985040776\n",
      "EM score =  0.0714285714286\n",
      "loss_train =  9.16797\n",
      "--------------------\n",
      "step =  116\n",
      "F1 Score =  0.0700211577812\n",
      "EM score =  0.0588235294118\n",
      "loss_train =  8.99828\n",
      "--------------------\n",
      "step =  117\n",
      "F1 Score =  0.0864794686925\n",
      "EM score =  0.0555555555556\n",
      "loss_train =  9.07137\n",
      "--------------------\n",
      "step =  118\n",
      "F1 Score =  0.0695763965017\n",
      "EM score =  0.0571428571429\n",
      "loss_train =  8.76318\n",
      "--------------------\n",
      "step =  119\n",
      "F1 Score =  0.0757901986347\n",
      "EM score =  0.0227272727273\n",
      "loss_train =  9.22273\n",
      "--------------------\n",
      "step =  120\n",
      "F1 Score =  0.0653925210761\n",
      "EM score =  0.0571428571429\n",
      "loss_train =  8.80668\n",
      "--------------------\n",
      "step =  121\n",
      "F1 Score =  0.0700404800086\n",
      "EM score =  0.137931034483\n",
      "loss_train =  9.2865\n",
      "--------------------\n",
      "step =  122\n",
      "F1 Score =  0.0580356945473\n",
      "EM score =  0.111111111111\n",
      "loss_train =  8.73828\n",
      "--------------------\n",
      "step =  123\n",
      "F1 Score =  0.069664935132\n",
      "EM score =  0.0512820512821\n",
      "loss_train =  9.18115\n",
      "--------------------\n",
      "step =  124\n",
      "F1 Score =  0.0639320740153\n",
      "EM score =  0.148148148148\n",
      "loss_train =  9.17869\n",
      "--------------------\n",
      "step =  125\n",
      "F1 Score =  0.120275344705\n",
      "EM score =  0.166666666667\n",
      "loss_train =  9.16044\n",
      "--------------------\n",
      "step =  126\n",
      "F1 Score =  0.0688943889615\n",
      "EM score =  0.0625\n",
      "loss_train =  9.07009\n",
      "--------------------\n",
      "step =  127\n",
      "F1 Score =  0.0534331248212\n",
      "EM score =  0.037037037037\n",
      "loss_train =  9.33972\n",
      "--------------------\n",
      "step =  128\n",
      "F1 Score =  0.0762704387609\n",
      "EM score =  0.133333333333\n",
      "loss_train =  9.49197\n",
      "--------------------\n",
      "step =  129\n",
      "F1 Score =  0.0650588000572\n",
      "EM score =  0.0625\n",
      "loss_train =  8.93181\n",
      "--------------------\n",
      "step =  130\n",
      "F1 Score =  0.0482323870084\n",
      "EM score =  0.0833333333333\n",
      "loss_train =  9.32697\n",
      "--------------------\n",
      "step =  131\n",
      "F1 Score =  0.0783574251195\n",
      "EM score =  0.133333333333\n",
      "loss_train =  9.35891\n",
      "--------------------\n",
      "step =  132\n",
      "F1 Score =  0.0987597051744\n",
      "EM score =  0.2\n",
      "loss_train =  9.37321\n",
      "--------------------\n",
      "step =  133\n",
      "F1 Score =  0.0642560016298\n",
      "EM score =  0.0909090909091\n",
      "loss_train =  9.45416\n",
      "--------------------\n",
      "step =  134\n",
      "F1 Score =  0.0639893181288\n",
      "EM score =  0.115384615385\n",
      "loss_train =  9.43528\n",
      "--------------------\n",
      "step =  135\n",
      "F1 Score =  0.0739500530809\n",
      "EM score =  0.0294117647059\n",
      "loss_train =  9.02659\n",
      "--------------------\n",
      "step =  136\n",
      "F1 Score =  0.0926091950724\n",
      "EM score =  0.105263157895\n",
      "loss_train =  8.96015\n",
      "--------------------\n",
      "step =  137\n",
      "F1 Score =  0.0585353971018\n",
      "EM score =  0.0769230769231\n",
      "loss_train =  9.16863\n",
      "--------------------\n",
      "step =  138\n",
      "F1 Score =  0.0457923279624\n",
      "EM score =  0.04\n",
      "loss_train =  9.62971\n",
      "--------------------\n",
      "step =  139\n",
      "F1 Score =  0.0894243985448\n",
      "EM score =  0.0263157894737\n",
      "loss_train =  8.79026\n",
      "--------------------\n",
      "step =  140\n",
      "F1 Score =  0.0813440840544\n",
      "EM score =  0.0857142857143\n",
      "loss_train =  9.12533\n",
      "--------------------\n",
      "step =  141\n",
      "F1 Score =  0.0773971575704\n",
      "EM score =  0.0967741935484\n",
      "loss_train =  9.03749\n",
      "--------------------\n",
      "step =  142\n",
      "F1 Score =  0.0763169611683\n",
      "EM score =  0.181818181818\n",
      "loss_train =  8.83608\n",
      "--------------------\n",
      "step =  143\n",
      "F1 Score =  0.0541384324121\n",
      "EM score =  0.105263157895\n",
      "loss_train =  9.30346\n",
      "--------------------\n",
      "step =  144\n",
      "F1 Score =  0.0590936406799\n",
      "EM score =  0.04\n",
      "loss_train =  8.73949\n",
      "--------------------\n",
      "step =  145\n",
      "F1 Score =  0.0686290368062\n",
      "EM score =  0.09375\n",
      "loss_train =  9.23897\n",
      "--------------------\n",
      "step =  146\n",
      "F1 Score =  0.0830602754097\n",
      "EM score =  0.107142857143\n",
      "loss_train =  9.69457\n",
      "--------------------\n",
      "step =  147\n",
      "F1 Score =  0.0540064428024\n",
      "EM score =  0.0\n",
      "loss_train =  9.17337\n",
      "--------------------\n",
      "step =  148\n",
      "F1 Score =  0.0611320737787\n",
      "EM score =  0.0909090909091\n",
      "loss_train =  9.41983\n",
      "--------------------\n",
      "step =  149\n",
      "F1 Score =  0.0787265619094\n",
      "EM score =  0.0967741935484\n",
      "loss_train =  9.53781\n",
      "--------------------\n",
      "step =  150\n",
      "F1 Score =  0.0648618706497\n",
      "EM score =  0.0333333333333\n",
      "loss_train =  9.26815\n",
      "--------------------\n",
      "step =  151\n",
      "F1 Score =  0.0571333542807\n",
      "EM score =  0.0714285714286\n",
      "loss_train =  9.04584\n",
      "--------------------\n",
      "step =  152\n",
      "F1 Score =  0.088684749993\n",
      "EM score =  0.153846153846\n",
      "loss_train =  9.11007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "step =  153\n",
      "F1 Score =  0.0844819667398\n",
      "EM score =  0.222222222222\n",
      "loss_train =  9.82338\n",
      "--------------------\n",
      "step =  154\n",
      "F1 Score =  0.0588327223321\n",
      "EM score =  0.0689655172414\n",
      "loss_train =  9.20692\n",
      "--------------------\n",
      "step =  155\n",
      "F1 Score =  0.0742896923247\n",
      "EM score =  0.103448275862\n",
      "loss_train =  9.08432\n",
      "--------------------\n",
      "step =  156\n",
      "F1 Score =  0.0591716585715\n",
      "EM score =  0.0285714285714\n",
      "loss_train =  9.45761\n",
      "--------------------\n",
      "step =  157\n",
      "F1 Score =  0.100786859214\n",
      "EM score =  0.166666666667\n",
      "loss_train =  9.28324\n",
      "--------------------\n",
      "step =  158\n",
      "F1 Score =  0.0685941327564\n",
      "EM score =  0.0740740740741\n",
      "loss_train =  9.27672\n",
      "--------------------\n",
      "step =  159\n",
      "F1 Score =  0.0768353421904\n",
      "EM score =  0.148148148148\n",
      "loss_train =  9.05223\n",
      "--------------------\n",
      "step =  160\n",
      "F1 Score =  0.0560842977306\n",
      "EM score =  0.0344827586207\n",
      "loss_train =  9.56123\n",
      "--------------------\n",
      "step =  161\n",
      "F1 Score =  0.0854476157846\n",
      "EM score =  0.259259259259\n",
      "loss_train =  9.27338\n"
     ]
    }
   ],
   "source": [
    "# quick validation, use last batch record in training set\n",
    "\n",
    "logger = Logger(\"./logs\")\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)\n",
    "    batch_size = 128\n",
    "    \n",
    "    for epocs in range(100):\n",
    "        counter = 0\n",
    "        for steps in range(636):\n",
    "            Q_batch = np.array(Q_data[counter:(counter+batch_size)])\n",
    "            D_batch = np.array(D_data[counter:(counter+batch_size)])\n",
    "            A_start_batch = np.array(A_start_data[counter:(counter+batch_size)])\n",
    "            A_end_batch = np.array(A_end_data[counter:(counter+batch_size)])\n",
    "            \n",
    "            # get prediction, evaluate score outside tensorflow (for simplicity)\n",
    "            _, loss_batch, pred_start_batch, pred_end_batch = sess.run([train_op, loss, pred_start, pred_end], \n",
    "                                                           feed_dict = {question : Q_batch, context : D_batch, answer_start : A_start_batch, answer_end : A_end_batch})\n",
    "            f1, em = evaluate(D_batch, pred_start_batch,pred_end_batch, A_start_batch, A_end_batch)\n",
    "            print (\"-\" * 20)\n",
    "            print (\"step = \", steps)\n",
    "            print (\"F1 Score = \", f1)\n",
    "            print (\"EM score = \", em)\n",
    "            print (\"loss_train = \", np.mean(loss_batch))\n",
    "            \n",
    "            # use PyTorch tensorboard logger, to log scalar outside of tensorflow\n",
    "            logger.scalar_summary('f1_train', f1, steps + epocs * 636)\n",
    "            logger.scalar_summary('em_train', em, steps + epocs * 636)\n",
    "            logger.scalar_summary('loss_train', np.mean(loss_batch), steps + epocs * 636)\n",
    "                \n",
    "            if steps % 100 == 0:\n",
    "                Q_val = np.array(Q_data[-batch_size:])\n",
    "                D_val = np.array(D_data[-batch_size:])\n",
    "                A_start = np.array(A_start_data[-batch_size:])\n",
    "                A_end = np.array(A_end_data[-batch_size:])\n",
    "                loss_val, pred_start_val, pred_end_val = sess.run([loss,pred_start,pred_end], feed_dict = {question : Q_val, context : D_val, answer_start : A_start, answer_end : A_end})\n",
    "                f1_val, em_val = evaluate(D_val,pred_start_val,pred_end_val, A_start,A_end)\n",
    "                print (\"=\"*20)\n",
    "                print (\"Validation Step = \", steps)\n",
    "                print (\"Validation F1 Score = \", f1_val)\n",
    "                print (\"Validation EM Score = \", em_val)\n",
    "                print (\"Validation loss = \", np.mean(loss_val))\n",
    "                \n",
    "                # log validation\n",
    "                logger.scalar_summary('f1_val', f1_val, steps + epocs * 636)\n",
    "                logger.scalar_summary('em_val', em_val, steps + epocs * 636)\n",
    "                logger.scalar_summary('loss_val', np.mean(loss_val), steps + epocs * 636)               \n",
    "                \n",
    "            counter = counter + batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Dynamic: see tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
